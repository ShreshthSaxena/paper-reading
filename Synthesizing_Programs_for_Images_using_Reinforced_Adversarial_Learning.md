# Synthesizing Programs for Images using Reinforced Adversarial Learning

## Abstract
For generative networks, use of graphic engines is benificial because they abstract away low level details and represent images as high level programs.
Current generative techniques are limiited by hand--crafted likelihood functions or distance functions. They have trained SPIRAL, an adversarially trained agent
that generates a program which is executed by a graphics engine to interpret and sample images using generator discriminator approach. The interesting add on here is to use 
discrimiator's output as a reward signal for meaningful learning progress.

## Introduction
- Humans learn to understand picture by decomposing it into strokes. Similarly, understanding a room through imagining it's layout.
- Inverse graphics: Given a scene, generate it's code. It is non-differnciable in general, so cant optimize directly. 
- Deep Reinforced Adversarial Learning: 
  1. An adversarially Trained Agent creates Visual program
  2. Visual Program Executed by Graphic Engine
  3. The Agent is optimized by being rewarded (RL Framework) for fooling discriminator
  4. Discriminator trained to distiinguish between rendered and real image
- The work's best contribution I saw was showing evidence that  utilizing a discriminatorâ€™s output as the
reward signal for reinforcement learning is significantly
better at optimizing the pixel error between renderings
and data, compared to directly optimizing pixel error
- Also, this approach is unsupervised, unlike sketch-rnn like stuff

## SPIRAL Agent
- There is an external black-box renderer that accepts sequence of actions a1, a2, ...aN and transforms them into the domai of interest, image of 3D rendering.
- Target data distribution for which we want Generator G is pd, we want to approximate it to output generated by rendered output of our generated sequence of actions pa.
- pa is generated by recurrent neural etwork PI (policy)
- For optimizing GAN, use wassersteins distance as it handles vastly dissimilar pg and pd more gracefully

For Generator
- Since output from pg (generated output) is created from Renderer, it isn't directly differencialble. Hence, naive gradient descent isn't possible.
The authors here have used maximization of expected return using RL (REINFORCE algo)
- The reward is set to 0 for time steps upto N and then for Nth timestep, it is score of discriminator on the generated image.
- Intermediate rewards could  be added to bias the search.
